{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd7fb27e",
   "metadata": {},
   "source": [
    "# Public LB 3.25997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c58332-02a4-44de-97e1-86047deaa7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54b133-f44c-4e11-8c51-36351c9ef000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73275cca-0660-4dcd-a452-d1fb04560dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything(113) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2336ab9-5007-4833-8104-3b013901f2d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv( 'test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91d690-bb9e-4fd5-8b48-7ed2744e7b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    df['Location'] = [re.sub(r'[^0-9a-zA-Z:,]',  ' ',str(i)) for i in df['Location']]\n",
    "    \n",
    "    df['City'] = [(i.split(',')[0]).lstrip().lower() for i in df['Location']]\n",
    "    df['State'] = [(i.split(',')[1]).lstrip().lower() for i in df['Location']]\n",
    "    df['Country'] = [(i.split(',')[2]).lstrip().lower() for i in df['Location']]\n",
    "\n",
    "    labels = ['0-3','3-6','6-8','8-12','12-18','18-25','25-34','35-44','45-54','55-64','65-74','75+']\n",
    "    bins = [0, 3, 6, 8, 12, 18, 25, 34, 44, 54, 64, 74, 250]\n",
    "    \n",
    "    # Age 이상치 처리\n",
    "    df['Age'] = df['Age'].apply(lambda x: 3 if x<3 else x)\n",
    "    df['Age'] = df['Age'].apply(lambda x: 100 if x>100 else x)\n",
    "\n",
    "    #df.loc[(df['Age'] > 90) | (df['Age'] < 3), 'Age'] = np.nan\n",
    "    \n",
    "    # 평균값으로 대체\n",
    "    #df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "    df['Age'] = df['Age'].astype(np.int32)\n",
    "\n",
    "    \n",
    "    df['Age_gb'] = pd.cut(df.Age, bins, labels = labels,include_lowest = True)\n",
    "    \n",
    "    # 출판년도 그룹화\n",
    "    # 만약 출판연도가 null이 있다면 정보없음(-1)로 채움\n",
    "    #df['Year-Of-Publication'] = df['Year-Of-Publication'].fillna(-1)\n",
    "    \n",
    "    #df.loc[(df['Year-Of-Publication'] > 2023) | (df['Age'] < 0), 'Year-Of-Publication'] = np.nan\n",
    "    #df['Year-Of-Publication'] = df['Year-Of-Publication'].fillna(df['Year-Of-Publication'].mean())\n",
    "    #df['Year-Of-Publication'] = df['Year-Of-Publication'].astype(np.int32)\n",
    "    \n",
    "    #labels = ['Unknown', '-1950', '1950-1960', '1960-1970', '1970-1980', '1980-1990', '1990-2000', '2000-2010', '2010-2020', '2020-']\n",
    "    #bins = [-1, 0, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020, 3000]\n",
    "    #df['Pub_gb'] = pd.cut(df['Year-Of-Publication'], bins, labels = labels,include_lowest = True)    \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a865d-810b-4cc7-bb96-83a33606c17f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re \n",
    "def preprocessing_data(df):\n",
    "    \n",
    "    # preprocessing Location\n",
    "    # NaN, N/A, etc.. Change 'unknown'\n",
    "    # Only using Train Data\t#\n",
    "    \n",
    "    # 최빈값을 사용하기 위해 새로운 데이터 프레임 생성(pd.Series.mode를 이용하면 같은 count수의 값을 list로 묶어서 정확하지 않음)\n",
    "    new_state = train_lb.groupby(['City'])['State'].value_counts().to_frame().rename(columns = {'State' : 'count'}).reset_index()\n",
    "    new_state = new_state[(~new_state['City'].isna())&(~new_state['State'].isna())&(new_state['count']!=1)]\n",
    "    new_state = new_state.sort_values(by=['City', 'count'], ascending=[True, False]).drop_duplicates(subset='City', keep='first')\n",
    "    new_state = new_state.rename(columns = {'State' : 'N_State'}) \n",
    "    new_state = new_state.drop(columns = ['count'])\n",
    "    \n",
    "    new_country = train_lb.groupby(['State'])['Country'].value_counts().to_frame().rename(columns = {'Country' : 'count'}).reset_index()\n",
    "    new_country = new_country[(~new_country['State'].isna())&(~new_country['Country'].isna())&(new_country['count']!=1)]\n",
    "    new_country = new_country.sort_values(by=['State', 'count'], ascending=[True, False]).drop_duplicates(subset='State', keep='first')\n",
    "    new_country = new_country.rename(columns = {'Country' : 'N_Country'}) \n",
    "    new_country = new_country.drop(columns = ['count'])\n",
    "    \n",
    "    df = pd.merge(df, new_country, on = 'State', how = 'left')\n",
    "    df = pd.merge(df, new_state, on = 'City', how = 'left')\n",
    "    \n",
    "    df['Country'] = np.where((df['Country'] == '')|(df['Country'].astype(str) == 'nan'), df['N_Country'], df['Country'])\n",
    "    df['State'] = np.where((df['State'] == '')|(df['State'].astype(str) == 'nan'), df['N_State'], df['State'])\n",
    "    df[['Country', 'State', 'City']] = df[['Country', 'State', 'City']].fillna(value= 'Unknown')\n",
    "    df = df.drop(columns = ['N_Country', 'N_State'])\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6093855-b175-45f6-9762-7f944b72c71f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "FEATURE = ['User-ID','Book-Title','Book-Author','Publisher', 'City','State','Country','Age_gb']#, 'Pub_gb']#, 'Language'] ', \n",
    "\n",
    "train_lb = train.__deepcopy__() \n",
    "test_lb = test.__deepcopy__()\n",
    "\n",
    "\n",
    "train_lb = feature_engineering(train_lb)\n",
    "test_lb = feature_engineering(test_lb)\n",
    "\n",
    "train_lb = preprocessing_data(train_lb)\n",
    "test_lb = preprocessing_data(test_lb)\n",
    "\n",
    "train_lb = train_lb.drop(columns = ['Book-ID', 'Location'])\n",
    "test_lb = test_lb.drop(columns = ['Book-ID', 'Location'])\n",
    "\n",
    "train_lb[FEATURE] = train_lb[FEATURE].astype(str) \n",
    "test_lb[FEATURE] = test_lb[FEATURE].astype(str)\n",
    "\n",
    "for i in FEATURE:\n",
    "    # train에는 없고, test에는 있는 원소는 -2 처리\n",
    "    oe = OrdinalEncoder(handle_unknown='use_encoded_value',\n",
    "                         unknown_value=-2)\n",
    "    oe=oe.fit(train_lb[i].to_numpy().reshape(-1, 1))\n",
    "    train_lb[i] = oe.transform(train_lb[i].to_numpy().reshape(-1, 1))\n",
    "    test_lb[i] = oe.transform(test_lb[i].to_numpy().reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc7008-acc0-4b76-9717-bb9d6ec16a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = train_lb.drop(columns = ['ID' , 'Book-Rating'])\n",
    "y_train = train_lb['Book-Rating']\n",
    "x_test = test_lb.drop(columns = ['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e934cc-1165-4ef9-8063-a8ee3dad701d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (7, 7))\n",
    "clustermap = sns.clustermap(X_train.corr(), cmap = 'RdYlBu_r',vmin = -1, vmax = 1, annot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073c5ec-ce37-4c5e-a2f6-24165699e41f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "model = CatBoostRegressor(random_seed = 113,\n",
    "                          l2_leaf_reg = 0.003426034644149707,\n",
    "                          max_bin = 358,\n",
    "                          subsample = 0.9974697184313627,\n",
    "                          learning_rate = 0.009464402227606937,\n",
    "                          max_depth = 11,\n",
    "                          min_data_in_leaf = 139,\n",
    "                          eval_metric = 'RMSE',\n",
    "                          iterations = 8694,\n",
    "                          task_type='GPU',\n",
    "                          bootstrap_type = 'Poisson',\n",
    "                          early_stopping_rounds = 100,\n",
    "                          verbose=500\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ed260-2d63-48cd-9386-0718fbb9d1a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "k = 20 # a number of folds best is 20\n",
    "skfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=113)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import r2_score\n",
    "from catboost import Pool\n",
    "\n",
    "y_valid_pred = 0*y_train\n",
    "y_test_pred = 0\n",
    "\n",
    "FEATURE = ['User-ID','Book-Title','Book-Author','Publisher', 'City','State','Country','Age_gb']#, 'Pub_gb']\n",
    "\n",
    "for i, (train_index, test_index) in tqdm(enumerate(skfold.split(X_train, y_train))):    \n",
    "    X_train_fold, X_valid_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_valid_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    X_train_fold[FEATURE] = X_train_fold[FEATURE].astype('int')\n",
    "    X_valid_fold[FEATURE] = X_valid_fold[FEATURE].astype('int')\n",
    "    \n",
    "    train_pool = Pool(data=X_train_fold, label=y_train_fold, cat_features=FEATURE)\n",
    "    valid_pool = Pool(data=X_valid_fold, label=y_valid_fold, cat_features=FEATURE)\n",
    "\n",
    "    \n",
    "    print( \"\\nFold \", i)\n",
    "    \n",
    "    fit_model = model.fit(train_pool, \n",
    "                          eval_set=valid_pool,\n",
    "                          use_best_model=True\n",
    "                          )\n",
    "    print( \"  N trees = \", model.tree_count_ )\n",
    "        \n",
    "\n",
    "    def score_model(model,X_train, X_test, y_train, y_test,\n",
    "               show_plot=True):   \n",
    "        y_pred = np.clip(model.predict(X_test),0,10)\n",
    "        print(f\"Test score: {r2_score(y_test, y_pred)}\")\n",
    "        print(\"MSE: \", mean_squared_error(y_test, y_pred))\n",
    "        print(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    \n",
    "        predictions_comparision = pd.DataFrame({'Actual': y_test.tolist(), 'Predicted': y_pred.tolist()}).sample(25)\n",
    "        if show_plot == True:\n",
    "            predictions_comparision.plot(kind=\"bar\", figsize=(12,8),title=\"Actual vs predicted values\")\n",
    "            print(predictions_comparision.sample(10))    \n",
    "    \n",
    "    \n",
    "        return {\n",
    "            \"test_score_r2\" : r2_score(y_test, y_pred),\n",
    "            \"test_score_mse\" : mean_squared_error(y_test, y_pred),\n",
    "            \"test_score_rmse\" : np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            }\n",
    "    score_model(fit_model, X_train_fold, X_valid_fold, y_train_fold, y_valid_fold, show_plot=True)\n",
    "    \n",
    "    x_test[FEATURE] = x_test[FEATURE].astype('int')\n",
    "    # Predict value Clipping\n",
    "    y_test_pred +=  np.clip(fit_model.predict(x_test[X_valid_fold.columns]),0.0,10.0)\n",
    "    \n",
    "y_test_pred /= k  # Average test set predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209276b-bb2e-406d-bc24-1f49c6b1aa8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_importance_feature = np.argsort(model.feature_importances_)[:-31:-1]\n",
    "plt.barh(X_train.columns[cat_importance_feature], model.feature_importances_[cat_importance_feature])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e595ee-2651-4f7d-9252-63f744101037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['ID'] = test['ID']\n",
    "sub['Book-Rating'] = y_test_pred\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801e9527-df5d-4ac3-b403-c6b2da58d24e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(sub['Book-Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cf777a-e7b1-44ea-a030-0f65a15f2fed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submit_cat+20-folds+best.csv', index=False,encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832b637-d1ef-4ecd-9abe-f09c2a1a4499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
