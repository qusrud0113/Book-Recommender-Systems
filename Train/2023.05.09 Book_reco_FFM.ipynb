{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca95e39c",
   "metadata": {},
   "source": [
    "# https://www.kaggle.com/code/leopoldvonranke/ffm-with-pytorch#Create-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2d2e0f-6d93-4275-9286-f32baa31e6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "641656dc-20d8-46d8-bd3f-9c6cb6b896bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64c29c8-122b-48d3-a2f2-6599a9d8687d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything(113) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6fbd9ed-5ade-49de-931a-d346cd2d2c42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv( 'test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1d64ae-8090-468d-9f5a-154373282403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessing_data(df):\n",
    "    # Decompose Location by split\n",
    "    df['City'] = [(i.split(',')[0]).lstrip().title() for i in df['Location']]\n",
    "    df['State'] = [(i.split(',')[1]).lstrip().title() for i in df['Location']]\n",
    "    df['Country'] = [(i.split(',')[2]).lstrip().title() for i in df['Location']]\n",
    "    \n",
    "    # preprocessing\n",
    "    # NaN, N/A, etc.. Change 'unknown'\n",
    "    # Only using Train Data\t#\n",
    "    #df['City'] = np.where((df['City'] == '')|(df['City'].astype(str) == 'nan')|(df['City'].astype(str) == 'N/A'), 'UnKnown', df['City'])\n",
    "    #df['State'] = np.where((df['State'] == '')|(df['State'].astype(str) == 'nan')|(df['State'].astype(str) == 'N/A'), 'UnKnown', df['State'])\n",
    "    #df['Country'] = np.where((df['Country'] == '')|(df['Country'].astype(str) == 'nan')|(df['Country'].astype(str) == 'N/A')\n",
    "    #                                              |(df['Country'].astype(str) == 'N/A - On The Road')|(df['Country'].astype(str) == 'X')\n",
    "    #                                              |(df['Country'].astype(str) == 'Far Away...')|(df['Country'].astype(str) == 'C')\n",
    "    #                                              |(df['Country'].astype(str) == 'We`Re Global!')|(df['Country'].astype(str) == 'Travelling....')\n",
    "    #                                              #영원한 중국!\n",
    "    #                                              |(df['Country'].astype(str) == 'La Chine Éternelle !')\n",
    "    #                         ,'UnKnown', df['Country'])\n",
    "\t#\n",
    "    ## train에 있는 정보로 County 바꾸기\n",
    "    #df.loc[df['State'] == 'Michigan Usa', 'Country'] = 'Usa'\n",
    "    #df.loc[df['State'] == 'Wicklow', 'Country'] = 'Ireland'\n",
    "    #df.loc[df['State'] == 'Ilfov', 'Country'] = ''\n",
    "    #df.loc[df['State'] == 'Liege', 'Country'] = 'Belgium'\n",
    "    #df.loc[df['State'] == 'Estremadura', 'Country'] = 'Portugal'\n",
    "    #df.loc[df['State'] == 'Aberdeenshire', 'Country'] = 'United Kingdom'\n",
    "    #df.loc[df['State'] == 'Wi', 'Country'] = 'Wisconsin'\n",
    "    #\n",
    "    ## 미국 Country name 통일\n",
    "    #df.loc[df['Country'] == 'America', 'Country'] = 'Usa'\n",
    "    #df.loc[df['Country'] == 'United State', 'Country'] = 'Usa'\n",
    "    #df.loc[df['Country'] == 'United States', 'Country'] = 'Usa'\n",
    "    #df.loc[df['Country'] == 'U.S.A.', 'Country'] = 'Usa'\n",
    "    #df.loc[df['Country'] == 'New York', 'Country'] = 'Usa'\n",
    "    #df.loc[df['Country'] == 'U.S. Of A.', 'Country'] = 'Usa'\n",
    "    #df.loc[df['Country'] == 'United Staes', 'Country'] = 'Usa'\n",
    "    #df.loc[df['Country'] == 'U.S>', 'Country'] = 'Usa'\n",
    "    #df.loc[df['Country'] == 'Usa Now', 'Country'] = 'Usa'\n",
    "    #\n",
    "    ## 영국 Country name 통일\n",
    "    #df.loc[df['Country'] == 'England', 'Country'] = 'United Kingdom'\n",
    "    #df.loc[df['Country'] == 'Scotland', 'Country'] = 'United Kingdom'\n",
    "    #df.loc[df['Country'] == 'Wales', 'Country'] = 'United Kingdom'\n",
    "    #df.loc[df['Country'] == 'Ireland', 'Country'] = 'United Kingdom'\n",
    "    #df.loc[df['Country'] == 'U.K.', 'Country'] = 'United Kingdom'\n",
    "    #df.loc[df['Country'] == 'Usa (Currently Living In England)', 'Country'] = 'United Kingdom'\n",
    "    #df.loc[df['Country'] == 'Uk', 'Country'] = 'United Kingdom'\n",
    "    #\n",
    "    ## 스페인 Country name 통일\n",
    "    #df.loc[df['Country'] == 'España ', 'Country'] = 'Spain'\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc3778e-3687-4baf-8d0e-8d8e532cacbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from lingua import Language, LanguageDetectorBuilder\n",
    "from tqdm import tqdm\n",
    "\n",
    "def feature_engineering(df):\n",
    "    # Book-Title로 도서 언어분류 (과적합 남)\n",
    "    #df['prep_title'] = df['Book-Title'].apply(preprocessing_sentence)\n",
    "    #detector = LanguageDetectorBuilder.from_all_languages().with_preloaded_language_models().with_low_accuracy_mode().build()\n",
    "    #\n",
    "    #all = []\n",
    "    #for row in tqdm(df['prep_title'].unique()):\n",
    "    #    try:\n",
    "    #        language = detector.detect_language_of(row)\n",
    "    #    except:\n",
    "    #        language = \"error\"\n",
    "    #        print(\"This row throws and error:\", row)\n",
    "    #    all.append(language)\n",
    "    #prep_title = pd.DataFrame(df['prep_title'].unique(), columns = ['prep_title'])\n",
    "    #all = [i.name if str(i) != 'None' else 'None' for i in all]\n",
    "    #Language = pd.DataFrame(all, columns = ['Language'])\n",
    "    #prep_title = pd.concat([prep_title, Language], axis = 1)\n",
    "    #df = pd.merge(df, prep_title, on = 'prep_title', how = 'inner')\n",
    "    #\n",
    "    #df = df.drop(columns = ['prep_title'])\n",
    "    \n",
    "    # Age 그룹화    \n",
    "    #labels = ['0-3','3-6','6-8','8-12','12-18','18-54','55-64','65+']\n",
    "    #bins = [0, 3, 6, 8, 12, 18, 54, 64, 250]\n",
    "    labels = ['0-3','3-6','6-8','8-12','12-18','18-25','25-34','35-44','45-54','55-64','65-74','75+']\n",
    "    bins = [0, 3, 6, 8, 12, 18, 25, 34, 44, 54, 64, 74, 250]\n",
    "    \n",
    "    # Age 이상치 처리\n",
    "    df['Age'] = df['Age'].apply(lambda x: 3 if x<3 else x)\n",
    "    df['Age'] = df['Age'].apply(lambda x: 100 if x>100 else x)\n",
    "    \n",
    "    df['Age_gb'] = pd.cut(df.Age, bins, labels = labels,include_lowest = True)\n",
    "    \n",
    "    # 출판년도 그룹화\n",
    "    #labels = ['Unknown', '-1900', '1900-1970', '1970-2000', '2000-2010', '2010-2020', '2020-']\n",
    "    #bins = [-1, 0, 1900, 1970, 2000, 2010, 2020, 3000]\n",
    "    labels = ['Unknown', '-1900', '1900-1950', '1950-1960', '1960-1970', '1970-1980', '1980-1990', '1990-2000', '2000-2010', '2010-2020', '2020-']\n",
    "    bins = [-1, 0, 1900, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020, 3000]\n",
    "    df['Pub_gb'] = pd.cut(df['Year-Of-Publication'], bins, labels = labels,include_lowest = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32129f41-b39b-4315-a174-c97ba8a3c66a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder,LabelEncoder\n",
    "\n",
    "FEATURE = ['User-ID','Book-Title','Book-Author','Publisher', 'City','State','Country','Age_gb', 'Pub_gb']#', 'Language'] ', \n",
    "\n",
    "train_lb = train.__deepcopy__() \n",
    "test_lb = test.__deepcopy__()\n",
    "\n",
    "train_lb = feature_engineering(train_lb)\n",
    "test_lb = feature_engineering(test_lb)\n",
    "\n",
    "train_lb = preprocessing_data(train_lb)\n",
    "test_lb = preprocessing_data(test_lb)\n",
    "\n",
    "train_lb = train_lb.drop(columns = ['Book-ID', 'Location'])\n",
    "test_lb = test_lb.drop(columns = ['Book-ID', 'Location'])\n",
    "\n",
    "train_lb[FEATURE] = train_lb[FEATURE].astype(str) \n",
    "test_lb[FEATURE] = test_lb[FEATURE].astype(str)\n",
    "\n",
    "\n",
    "for i in FEATURE:\n",
    "    le = LabelEncoder()\n",
    "    le=le.fit(train_lb[i])\n",
    "    for label in np.unique(test_lb[i].dropna()):\n",
    "        if label not in le.classes_: \n",
    "            le.classes_ = np.append(le.classes_, label)\n",
    "    train_lb[i] = le.transform(train_lb[i])\n",
    "    test_lb[i] = le.transform(test_lb[i])\n",
    "    \n",
    "#for i in FEATURE:\n",
    "#    # train에는 없고, test에는 있는 원소는 -2 처리\n",
    "#    oe = OrdinalEncoder(handle_unknown='use_encoded_value',\n",
    "#                         unknown_value=-2)\n",
    "#    oe=oe.fit(train_lb[i].to_numpy().reshape(-1, 1))\n",
    "#    train_lb[i] = oe.transform(train_lb[i].to_numpy().reshape(-1, 1))\n",
    "#    test_lb[i] = oe.transform(test_lb[i].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cb76bfa-089c-4362-ba78-5a090c846f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_lb.to_csv('train_lb.csv')\n",
    "test_lb.to_csv('test_lb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3641bf9-5444-4567-a407-25d7d85cc1f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_lb = pd.read_csv('train_lb.csv')\n",
    "test_lb = pd.read_csv('test_lb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47d4f4fa-fabc-4572-9099-8286bfd0fa2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = train_lb.drop(columns = ['Unnamed: 0','ID', 'Book-Rating', 'Age', 'Year-Of-Publication'])\n",
    "y_train = train_lb['Book-Rating']\n",
    "x_test = test_lb.drop(columns = ['Unnamed: 0','ID', 'Age', 'Year-Of-Publication'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "252de616-54fb-4365-85a2-8990af4347f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FeaturesLinear(torch.nn.Module) :\n",
    "    def __init__(self, field_dims) :\n",
    "        '''\n",
    "        Parameter\n",
    "            field_dims : List of field dimensions\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.input_dim = sum(field_dims)\n",
    "        self.linear = nn.Linear(self.input_dim, 1, bias=True)\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Parameter\n",
    "            x : Long tensor of size (batch_size, num_fields)\n",
    "        \n",
    "        Return\n",
    "            linear_term : Float tensor of size (batch_size)\n",
    "        '''\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        sparse_x = torch.zeros(x.size(0), self.input_dim, device=x.device).scatter_(1, x, 1.)\n",
    "        linear_term = self.linear(sparse_x)\n",
    "        return linear_term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d29ad94b-004d-4bed-8b91-e4ece4d476ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FieldAwareEmbedding(torch.nn.Module) :\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim) :\n",
    "        '''\n",
    "        Parameter\n",
    "            field_dims : List of field dimensions\n",
    "            embed_dim : Factorization dimension for dense embedding\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(feature_size, embed_dim) for feature_size in field_dims\n",
    "        ])\n",
    "        for embedding in self.embeddings:\n",
    "            torch.nn.init.xavier_uniform_(embedding.weight.data)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        '''\n",
    "        Parameter\n",
    "            x : Long tensor of size (batch_size, num_fields)\n",
    "        \n",
    "        Return\n",
    "            dense_x : Long tensor of size (batch_size, num_fields, embed_dim)\n",
    "        '''\n",
    "        dense_x = [self.embeddings[i](x[..., i]) for i in range(self.num_fields)]\n",
    "        dense_x = torch.stack(dense_x, dim=1)\n",
    "        return dense_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e972794c-aae0-4ab3-bc83-2c4b06199a15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FieldAwareFactorizationMachine(torch.nn.Module) :\n",
    "    def __init__(self, field_dims, embed_dim) :\n",
    "        '''\n",
    "        Parameter\n",
    "            field_dims : List of field dimensions\n",
    "            embed_dim : Factorization dimension for dense embedding\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embeddings = FieldAwareEmbedding(field_dims, embed_dim)\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "        \n",
    "    def square(self, x):\n",
    "        return torch.pow(x,2)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        '''\n",
    "        Parameter\n",
    "            x : Long tensor of size (batch_size, num_fields)\n",
    "        \n",
    "        Return\n",
    "            y_ffm : Float tensor of size (batch_size)\n",
    "        '''\n",
    "        linear_term = self.linear(x)\n",
    "        \n",
    "        dense_x = self.embeddings(x)\n",
    "\n",
    "        square_of_sum = self.square(torch.sum(dense_x, dim=1))\n",
    "        sum_of_square = torch.sum(self.square(dense_x), dim=1)\n",
    "        pairwise_term = 0.5 * torch.sum(square_of_sum - sum_of_square, dim=1)\n",
    "        \n",
    "        y_ffm = linear_term.squeeze(1) + pairwise_term\n",
    "\n",
    "        return y_ffm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a58fc88-2873-42a9-811e-9f3457deb1cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_tr, X_valid, y_tr, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42,\n",
    "                                                      shuffle=True, stratify=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "277602ff-489a-4f33-baf0-fa3b554d3a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.LongTensor(X_tr.to_numpy()),\n",
    "                              torch.LongTensor(y_tr.to_numpy()))\n",
    "valid_dataset = TensorDataset(torch.LongTensor(X_valid.to_numpy()),\n",
    "                              torch.LongTensor(y_valid.to_numpy()))\n",
    "\n",
    "test_dataset = TensorDataset(torch.LongTensor(x_test.to_numpy()),\n",
    "                              torch.LongTensor(np.zeros(x_test.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28e44fe5-20eb-45c5-b32c-9c156903ce95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.eps = 1e-6\n",
    "    def forward(self, x, y):\n",
    "        criterion = MSELoss()\n",
    "        loss = torch.sqrt(criterion(x, y)+self.eps)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52fe6f0c-9ac5-4ac2-a4ab-41c4a53e2fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_train(model, dataloader, loss_fn, optimizer, device) :\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, targets in tqdm(dataloader) :\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(torch.float32).to(device)\n",
    "        \n",
    "        outputs = model(features)\n",
    "        loss = loss_fn(targets, outputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        \n",
    "    return (total_loss/len(dataloader)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5c86d8b-3562-43d8-846d-d3b4de433062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_eval(model, dataloader, loss_fn, device) :\n",
    "    with torch.no_grad() :\n",
    "        total_loss = 0\n",
    "        model.eval()\n",
    "        for features, targets in tqdm(dataloader) :\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(torch.float32).to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            loss = loss_fn(targets, outputs)\n",
    "            total_loss += loss\n",
    "            \n",
    "    return (total_loss/len(dataloader)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "967797ee-1501-4269-b5a8-e95fd5d7e5af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d62ba9a-9256-4e2a-b6b5-798e9f6fe119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = X_train.columns\n",
    "idx = {feature:None for feature in features}\n",
    "for feature in features :\n",
    "    feature2idx = {v:k for k,v in enumerate(X_train[feature].unique())}\n",
    "    idx[feature] = len(feature2idx)\n",
    "    X_train[feature] = X_train[feature].map(feature2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbda6baa-1463-4592-8496-0710e5ec8a15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 83256 217829  92635  15505     12     11  13820   1810    348]\n"
     ]
    }
   ],
   "source": [
    "field_dims = np.array(list(idx.values()), dtype=np.uint32)\n",
    "print(field_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1ce0484-1eae-4b8b-9657-858957ec74c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "embed_dim = 4\n",
    "batch_size = 128\n",
    "lr = 0.0001\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9531b7d-bf3b-492d-8b88-a28361b6dc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5447/5447 [00:44<00:00, 122.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1362/1362 [00:04<00:00, 306.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss : 4.01, Validation Loss : 3.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5447/5447 [00:44<00:00, 121.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1362/1362 [00:04<00:00, 308.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Train Loss : 3.58, Validation Loss : 3.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5447/5447 [00:52<00:00, 104.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1362/1362 [00:04<00:00, 280.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Train Loss : 3.46, Validation Loss : 3.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████▏                                              | 2174/5447 [00:18<00:27, 119.22it/s]"
     ]
    }
   ],
   "source": [
    "ffm_model = FieldAwareFactorizationMachine(field_dims, embed_dim=embed_dim)\n",
    "ffm_model.to(device)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "loss_fn = RMSELoss()\n",
    "optimizer = torch.optim.Adam(ffm_model.parameters(), lr=lr)\n",
    "\n",
    "best_loss = int(1e8)\n",
    "check_cnt = 0\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "    train_loss = model_train(ffm_model, train_loader, loss_fn, optimizer, device)\n",
    "    valid_loss = model_eval(ffm_model, valid_loader, loss_fn, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"Train Loss : {train_loss:.2f}, Validation Loss : {valid_loss:.2f}\")\n",
    "    \n",
    "    if best_loss > valid_loss :\n",
    "        best_loss = valid_loss\n",
    "        torch.save(ffm_model.state_dict(), 'ffm_model.pt')\n",
    "        check_cnt = 0\n",
    "    else :\n",
    "        check_cnt += 1\n",
    "        if check_cnt == 5 :\n",
    "            print(\"Early Stopped\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cefffcc-0268-4cb5-9e8e-9f662cbd2cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = FieldAwareFactorizationMachine(field_dims, embed_dim).to(device)\n",
    "best_model.load_state_dict(torch.load('ffm_model.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
